{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deadly-argentina",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "In this notebook, I will work through an implementation of the KNN algorithm. This implementation will cover both regression and classification use cases. I will use the breast cancer and diabetes datasets, available from scikit-learn, to test this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developing-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from typing import Dict, Any\n",
    "from abc import ABC,abstractmethod\n",
    "from sklearn.datasets import load_diabetes, load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.metrics import mean_squared_error,\\\n",
    "                            mean_absolute_error,\\\n",
    "                            accuracy_score,\\\n",
    "                            precision_score,\\\n",
    "                            recall_score,\\\n",
    "                            f1_score,\\\n",
    "                            make_scorer\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a67887",
   "metadata": {},
   "source": [
    "Now let's proceed to develop our implementation of KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16392eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN(ABC):\n",
    "    \"\"\"\n",
    "    Base class for KNN implementations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K : int = 3, metric : str = 'minkowski', p : int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Initializer function. Ensure that input parameters are compatiable.\n",
    "        Inputs:\n",
    "            K      -> integer specifying number of neighbours to consider\n",
    "            metric -> string to indicate the distance metric to use (valid entries are 'minkowski' or 'cosine')\n",
    "            p      -> order of the minkowski metric (valid only when distance == 'minkowski')\n",
    "        \"\"\"\n",
    "        # check distance is a valid entry\n",
    "        valid_distance = ['minkowski','cosine']\n",
    "        if metric not in valid_distance:\n",
    "            msg = \"Entered value for metric is not valid. Pick one of {}\".format(valid_distance)\n",
    "            raise ValueError(msg)\n",
    "        # check minkowski p parameter\n",
    "        if (metric == 'minkowski') and (p <= 0):\n",
    "            msg = \"Entered value for p is not valid. For metric = 'minkowski', p >= 1\"\n",
    "            raise ValueError(msg)\n",
    "        # store/initialise input parameters\n",
    "        self.K       = K\n",
    "        self.metric  = metric\n",
    "        self.p       = p\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        \n",
    "    def __del__(self) -> None:\n",
    "        \"\"\"\n",
    "        Destructor function. \n",
    "        \"\"\"\n",
    "        del self.K\n",
    "        del self.metric\n",
    "        del self.p\n",
    "        del self.X_train\n",
    "        del self.y_train\n",
    "      \n",
    "    def __minkowski(self, x : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Private function to compute the minkowski distance between point x and the training data X\n",
    "        Inputs:\n",
    "            x -> numpy data point of predictors to consider\n",
    "        Outputs:\n",
    "            np.array -> numpy array of the computed distances\n",
    "        \"\"\"\n",
    "        return np.power(np.sum(np.power(np.abs(self.X_train - x),self.p),axis=1),1/self.p)\n",
    "    \n",
    "    def __cosine(self, x : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Private function to compute the cosine distance between point x and the training data X\n",
    "        Inputs:\n",
    "            x -> numpy data point of predictors to consider\n",
    "        Outputs:\n",
    "            np.array -> numpy array of the computed distances\n",
    "        \"\"\"\n",
    "        return (1 - (np.dot(self.X_train,x)/(np.linalg.norm(x)*np.linalg.norm(self.X_train,axis=1))))\n",
    "    \n",
    "    def __distances(self, X : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Private function to compute distances to each point x in X[x,:]\n",
    "        Inputs:\n",
    "            X -> numpy array of points [x]\n",
    "        Outputs:\n",
    "            D -> numpy array containing distances from x to all points in the training set.\n",
    "        \"\"\"\n",
    "        # cover distance calculation\n",
    "        if self.metric == 'minkowski':\n",
    "            D = np.apply_along_axis(self.__minkowski,1,X)\n",
    "        elif self.metric == 'cosine':\n",
    "            D = np.apply_along_axis(self.__cosine,1,X)\n",
    "        # return computed distances\n",
    "        return D\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _generate_predictions(self, idx_neighbours : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Protected function to compute predictions from the K nearest neighbours\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X : np.array, y : np.array) -> None:\n",
    "        \"\"\"\n",
    "        Public training function for the class. It is assummed input X has been normalised.\n",
    "        Inputs:\n",
    "            X -> numpy array containing the predictor features\n",
    "            y -> numpy array containing the labels associated with each value in X\n",
    "        \"\"\"\n",
    "        # store training data\n",
    "        self.X_train = np.copy(X)\n",
    "        self.y_train = np.copy(y)\n",
    "        \n",
    "    def predict(self, X : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Public prediction function for the class. \n",
    "        It is assummed input X has been normalised in the same fashion as the input to the training function\n",
    "        Inputs:\n",
    "            X -> numpy array containing the predictor features\n",
    "        Outputs:\n",
    "           y_pred -> numpy array containing the predicted labels\n",
    "        \"\"\"\n",
    "        # ensure we have already trained the instance\n",
    "        if (self.X_train.size == 0) or (self.y_train.size == 0):\n",
    "            raise Exception('Model is not trained. Call fit before calling predict.')\n",
    "        # compute distances\n",
    "        D = self.__distances(X)\n",
    "        # obtain indices for the K nearest neighbours\n",
    "        idx_neighbours = D.argsort()[:,:self.K]\n",
    "        # compute predictions\n",
    "        y_pred = self._generate_predictions(idx_neighbours)\n",
    "        # return results\n",
    "        return y_pred\n",
    "    \n",
    "    def get_params(self, deep : bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Public function to return model parameters\n",
    "        Inputs:\n",
    "            deep -> boolean input parameter\n",
    "        Outputs:\n",
    "            Dict -> dictionary of stored class input parameters\n",
    "        \"\"\"\n",
    "        return {'K':self.K,\n",
    "                'metric':self.metric,\n",
    "                'p':self.p}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e3b6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier(KNN):\n",
    "    \"\"\"\n",
    "    Class for KNN classifiction implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K : int = 3, metric : str = 'minkowski', p : int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Initializer function. Ensure that input parameters are compatiable.\n",
    "        Inputs:\n",
    "            K       -> integer specifying number of neighbours to consider\n",
    "            metric  -> string to indicate the distance metric to use (valid entries are 'minkowski' or 'cosine')\n",
    "            p       -> order of the minkowski metric (valid only when distance == 'minkowski')\n",
    "        \"\"\"\n",
    "        # call base class initialiser\n",
    "        super().__init__(K,metric,p)\n",
    "        \n",
    "    def _generate_predictions(self, idx_neighbours : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Protected function to compute predictions from the K nearest neighbours\n",
    "        Inputs:\n",
    "            idx_neighbours -> indices of nearest neighbours\n",
    "        Outputs:\n",
    "            y_pred -> numpy array of prediction results\n",
    "        \"\"\"        \n",
    "        # compute the mode label for each submitted sample\n",
    "        y_pred = stats.mode(self.y_train[idx_neighbours],axis=1).mode.flatten()   \n",
    "        # return result\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7bf3053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNRegressor(KNN):\n",
    "    \"\"\"\n",
    "    Class for KNN regression implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, K : int = 3, metric : str = 'minkowski', p : int = 2) -> None:\n",
    "        \"\"\"\n",
    "        Initializer function. Ensure that input parameters are compatiable.\n",
    "        Inputs:\n",
    "            K       -> integer specifying number of neighbours to consider\n",
    "            metric  -> string to indicate the distance metric to use (valid entries are 'minkowski' and 'cosine')\n",
    "            p       -> order of the minkowski metric (valid only when distance == 'minkowski')\n",
    "        \"\"\"\n",
    "        # call base class initialiser\n",
    "        super().__init__(K,metric,p)\n",
    "        \n",
    "    def _generate_predictions(self, idx_neighbours : np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        Protected function to compute predictions from the K nearest neighbours\n",
    "        Inputs:\n",
    "            idx_neighbours -> indices of nearest neighbours\n",
    "        Outputs:\n",
    "            y_pred -> numpy array of prediction results\n",
    "        \"\"\"\n",
    "        # compute the mean label for each submitted sample\n",
    "        y_pred = np.mean(self.y_train[idx_neighbours],axis=1)         \n",
    "        # return result\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2127f",
   "metadata": {},
   "source": [
    "## KNN Classification\n",
    "\n",
    "### Load Classification Dataset\n",
    "\n",
    "Here I'll load the breast cancer dataset. A full description of these data can be found at: https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset. \n",
    "\n",
    "Note I already analysed these data in Notebook II - Logistic Regression. As such, I won't repeat that work here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "010bd161",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load classification dataset ##\n",
    "data = load_breast_cancer()\n",
    "X    = data.data\n",
    "y    = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "169164d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# properly format labels\n",
    "y = np.where(y==0,-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a9ccef",
   "metadata": {},
   "source": [
    "### Investigate Performance\n",
    "\n",
    "Here I will use 10-fold cross-validation to measure the performance of the KNN classifier. We will also try a variety of values for K & the distance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccba9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the scoring metrics ##\n",
    "scoring_metrics = {'accuracy' : make_scorer(accuracy_score), \n",
    "                   'precision': make_scorer(precision_score),\n",
    "                   'recall'   : make_scorer(recall_score),\n",
    "                   'f1'       : make_scorer(f1_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df9de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a helper function for our analysis ##\n",
    "def cv_classifier_analysis(pipe : Any, \n",
    "                           X : np.array, \n",
    "                           y : np.array, \n",
    "                           k : int, \n",
    "                           scoring_metrics : Dict,\n",
    "                           metric : str) -> None:\n",
    "    \"\"\"\n",
    "    Function to carry out cross-validation analysis for input KNN classifier\n",
    "    Inputs:\n",
    "        pipe            -> input pipeline containing preprocessing and KNN classifier\n",
    "        X               -> numpy array of predictors\n",
    "        y               -> numpy array of labels\n",
    "        k               -> integer value for number of nearest neighbours to consider\n",
    "        scoring_metrics -> dictionary of scoring metrics to consider \n",
    "        metric          -> string indicating distance metric used\n",
    "    \"\"\"\n",
    "    # print hyperparameter configuration\n",
    "    print('RESULTS FOR K = {0}, {1}'.format(k,metric))\n",
    "    # run cross validation\n",
    "    dcScores = cross_validate(pipe,X,y,cv=StratifiedKFold(10),scoring=scoring_metrics)\n",
    "    # report results\n",
    "    print('Mean Accuracy: %.2f' % np.mean(dcScores['test_accuracy']))\n",
    "    print('Mean Precision: %.2f' % np.mean(dcScores['test_precision']))\n",
    "    print('Mean Recall: %.2f' % np.mean(dcScores['test_recall']))\n",
    "    print('Mean F1: %.2f' % np.mean(dcScores['test_f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "142b2d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR K = 3, MANHATTEN DISTANCE\n",
      "Mean Accuracy: 0.97\n",
      "Mean Precision: 0.97\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.98\n",
      "RESULTS FOR K = 3, EUCLIDEAN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 3, COSINE DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.97\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 6, MANHATTEN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.98\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 6, EUCLIDEAN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.98\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 6, COSINE DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.97\n",
      "Mean Recall: 0.96\n",
      "Mean F1: 0.96\n",
      "RESULTS FOR K = 9, MANHATTEN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 9, EUCLIDEAN DISTANCE\n",
      "Mean Accuracy: 0.97\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 9, COSINE DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.98\n",
      "Mean F1: 0.97\n"
     ]
    }
   ],
   "source": [
    "## perform cross-validation for a range of model hyperparameters for the Custom model ##\n",
    "K = [3,6,9]\n",
    "for k in K:\n",
    "    # define the pipeline for manhatten distance\n",
    "    p_manhat = Pipeline([('scaler', StandardScaler()), ('knn', KNNClassifier(k, metric = 'minkowski', p = 1))])\n",
    "    # define the pipeline for euclidean distance\n",
    "    p_euclid = Pipeline([('scaler', StandardScaler()), ('knn', KNNClassifier(k, metric = 'minkowski', p = 2))])\n",
    "    # define the pipeline for cosine distance\n",
    "    p_cosine = Pipeline([('scaler', StandardScaler()), ('knn', KNNClassifier(k, metric = 'cosine'))])\n",
    "    # cross validate for p_manhat\n",
    "    cv_classifier_analysis(p_manhat, X, y, k, scoring_metrics, 'MANHATTEN DISTANCE')\n",
    "    # cross validate for p_euclid\n",
    "    cv_classifier_analysis(p_euclid, X, y, k, scoring_metrics, 'EUCLIDEAN DISTANCE')\n",
    "    # cross validate for p_cosine\n",
    "    cv_classifier_analysis(p_cosine, X, y, k, scoring_metrics, 'COSINE DISTANCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4d6f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR K = 3, MANHATTEN DISTANCE\n",
      "Mean Accuracy: 0.97\n",
      "Mean Precision: 0.97\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.98\n",
      "RESULTS FOR K = 3, EUCLIDEAN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 3, COSINE DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.97\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 6, MANHATTEN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.98\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 6, EUCLIDEAN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.98\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 6, COSINE DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.97\n",
      "Mean Recall: 0.96\n",
      "Mean F1: 0.96\n",
      "RESULTS FOR K = 9, MANHATTEN DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 9, EUCLIDEAN DISTANCE\n",
      "Mean Accuracy: 0.97\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.99\n",
      "Mean F1: 0.97\n",
      "RESULTS FOR K = 9, COSINE DISTANCE\n",
      "Mean Accuracy: 0.96\n",
      "Mean Precision: 0.96\n",
      "Mean Recall: 0.98\n",
      "Mean F1: 0.97\n"
     ]
    }
   ],
   "source": [
    "## perform cross-validation for a range of model hyperparameters for the Scikit-learn model ##\n",
    "K = [3,6,9]\n",
    "for k in K:\n",
    "    # define the model for manhatten distance\n",
    "    p_manhat = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(k, metric = 'minkowski', p = 1))])\n",
    "    # define the model for euclidean distance\n",
    "    p_euclid = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(k, metric = 'minkowski', p = 2))])\n",
    "    # define the model for cosine distance\n",
    "    p_cosine = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsClassifier(k, metric = 'cosine'))])\n",
    "    # cross validate for m_manhat\n",
    "    cv_classifier_analysis(p_manhat, X, y, k, scoring_metrics, 'MANHATTEN DISTANCE')\n",
    "    # cross validate for m_euclid\n",
    "    cv_classifier_analysis(p_euclid, X, y, k, scoring_metrics, 'EUCLIDEAN DISTANCE')\n",
    "    # cross validate for m_cosine\n",
    "    cv_classifier_analysis(p_cosine, X, y, k, scoring_metrics, 'COSINE DISTANCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caded59f",
   "metadata": {},
   "source": [
    "We can better summarise these results in a table:\n",
    "\n",
    "K | Distance | Custom Accuracy | Sklearn Accuracy | Custom Precision | Sklearn Precision | Custom Recall | Sklearn Recall | Custom F1 | Sklearn F1\n",
    "--- | --- | --- | --- | --- | --- | --- | --- | --- | --- \n",
    "3 | Manhatten | 0.97 | 0.97 | 0.97 | 0.97 | 0.99 | 0.99 | 0.98 | 0.98\n",
    "3 | Euclidean | 0.96 | 0.96 | 0.96 | 0.96 | 0.99 | 0.99 | 0.97 | 0.97\n",
    "3 | Cosine | 0.96 | 0.96 | 0.96 | 0.96 | 0.97 | 0.97 | 0.97 | 0.97\n",
    "6 | Manhatten | 0.96 | 0.96 | 0.96 | 0.96 | 0.98 | 0.98 | 0.97 | 0.97\n",
    "6 | Euclidean | 0.96 | 0.96 | 0.96 | 0.96 | 0.98 | 0.98 | 0.97 | 0.97\n",
    "6 | Cosine | 0.96 | 0.96 | 0.97 | 0.97 | 0.96 | 0.96 | 0.96 | 0.96\n",
    "9 | Manhatten | 0.96 | 0.96 | 0.96 | 0.96 | 0.99 | 0.99 | 0.97 | 0.97\n",
    "9 | Euclidean | 0.97 | 0.97 | 0.96 | 0.96 | 0.99 | 0.99 | 0.97 | 0.97\n",
    "9 | Cosine | 0.96 | 0.96 | 0.96 | 0.96 | 0.98 | 0.98 | 0.97 | 0.97\n",
    "\n",
    "Firstly, it's clear that our custom KNN classifier yields results that are identicial to the scikit-learn implementation. Looking at the statistics tabulated, it appears that using the Manhatten distance with $K = 3$ produces the best results. However, it should be clear that performance of the KNN classifiers appear to vary little with the choice of hyperparameters analysed here.\n",
    "\n",
    "## KNN Regression\n",
    "\n",
    "### Load Regression Dataset\n",
    "\n",
    "Here I'll load the diabetes dataset, available from scikit-learn. A full description of this dataset is available here: https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset.\n",
    "\n",
    "Note I have already explored these data in Notebook XI - Adaboost Regression. As such, I won't repeat it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f364a974",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load regression dataset ##\n",
    "X,y = load_diabetes(return_X_y=True,as_frame=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e14aca",
   "metadata": {},
   "source": [
    "### Investigate Performance\n",
    "\n",
    "Here I will use 10-fold cross-validation to measure the performance of the KNN regressor. We will also try a variety of values for K & the distance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e69b0119",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the scoring metrics ##\n",
    "scoring_metrics = {'mse' : make_scorer(mean_squared_error), \n",
    "                   'mae': make_scorer(mean_absolute_error)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1049e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a helper function for our analysis ##\n",
    "def cv_regressor_analysis(pipe : Any, \n",
    "                          X : np.array, \n",
    "                          y : np.array, \n",
    "                          k : int, \n",
    "                          scoring_metrics : Dict,\n",
    "                          metric : str) -> None:\n",
    "    \"\"\"\n",
    "    Function to carry out cross-validation analysis for input KNN regressor\n",
    "    Inputs:\n",
    "        pipe            -> input pipeline containing preprocessing and KNN regressor\n",
    "        X               -> numpy array of predictors\n",
    "        y               -> numpy array of labels\n",
    "        k               -> integer value for number of nearest neighbours to consider\n",
    "        scoring_metrics -> dictionary of scoring metrics to consider \n",
    "        metric          -> string indicating distance metric used\n",
    "    \"\"\"\n",
    "    # print hyperparameter configuration\n",
    "    print('RESULTS FOR K = {0}, {1}'.format(k,metric))\n",
    "    # run cross validation\n",
    "    dcScores = cross_validate(pipe,X,y,cv=10,scoring=scoring_metrics)\n",
    "    # report results\n",
    "    print('Mean MSE: %.2f' % np.mean(dcScores['test_mse']))\n",
    "    print('Mean MAE: %.2f' % np.mean(dcScores['test_mae']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3804bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR K = 3, MANHATTEN DISTANCE\n",
      "Mean MSE: 3934.75\n",
      "Mean MAE: 48.82\n",
      "RESULTS FOR K = 3, EUCLIDEAN DISTANCE\n",
      "Mean MSE: 4087.34\n",
      "Mean MAE: 49.00\n",
      "RESULTS FOR K = 3, COSINE DISTANCE\n",
      "Mean MSE: 3814.75\n",
      "Mean MAE: 47.08\n",
      "RESULTS FOR K = 6, MANHATTEN DISTANCE\n",
      "Mean MSE: 3598.75\n",
      "Mean MAE: 48.01\n",
      "RESULTS FOR K = 6, EUCLIDEAN DISTANCE\n",
      "Mean MSE: 3640.96\n",
      "Mean MAE: 47.55\n",
      "RESULTS FOR K = 6, COSINE DISTANCE\n",
      "Mean MSE: 3483.80\n",
      "Mean MAE: 45.79\n",
      "RESULTS FOR K = 9, MANHATTEN DISTANCE\n",
      "Mean MSE: 3504.69\n",
      "Mean MAE: 47.05\n",
      "RESULTS FOR K = 9, EUCLIDEAN DISTANCE\n",
      "Mean MSE: 3451.70\n",
      "Mean MAE: 46.59\n",
      "RESULTS FOR K = 9, COSINE DISTANCE\n",
      "Mean MSE: 3403.23\n",
      "Mean MAE: 46.06\n"
     ]
    }
   ],
   "source": [
    "## perform cross-validation for a range of model hyperparameters for the Custom model ##\n",
    "K = [3,6,9]\n",
    "for k in K:       \n",
    "    # define the pipeline for manhatten distance\n",
    "    p_manhat = Pipeline([('scaler', StandardScaler()), ('knn', KNNRegressor(k, metric = 'minkowski', p = 1))])\n",
    "    # define the pipeline for euclidean distance\n",
    "    p_euclid = Pipeline([('scaler', StandardScaler()), ('knn', KNNRegressor(k, metric = 'minkowski', p = 2))])\n",
    "    # define the pipeline for cosine distance\n",
    "    p_cosine = Pipeline([('scaler', StandardScaler()), ('knn', KNNRegressor(k, metric = 'cosine'))])\n",
    "    # cross validate for p_manhat\n",
    "    cv_regressor_analysis(p_manhat, X, y, k, scoring_metrics, 'MANHATTEN DISTANCE')\n",
    "    # cross validate for p_euclid\n",
    "    cv_regressor_analysis(p_euclid, X, y, k, scoring_metrics, 'EUCLIDEAN DISTANCE')\n",
    "    # cross validate for p_cosine\n",
    "    cv_regressor_analysis(p_cosine, X, y, k, scoring_metrics, 'COSINE DISTANCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41020f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR K = 3, MANHATTEN DISTANCE\n",
      "Mean MSE: 3934.75\n",
      "Mean MAE: 48.82\n",
      "RESULTS FOR K = 3, EUCLIDEAN DISTANCE\n",
      "Mean MSE: 4087.34\n",
      "Mean MAE: 49.00\n",
      "RESULTS FOR K = 3, COSINE DISTANCE\n",
      "Mean MSE: 3814.75\n",
      "Mean MAE: 47.08\n",
      "RESULTS FOR K = 6, MANHATTEN DISTANCE\n",
      "Mean MSE: 3598.75\n",
      "Mean MAE: 48.01\n",
      "RESULTS FOR K = 6, EUCLIDEAN DISTANCE\n",
      "Mean MSE: 3640.96\n",
      "Mean MAE: 47.55\n",
      "RESULTS FOR K = 6, COSINE DISTANCE\n",
      "Mean MSE: 3483.80\n",
      "Mean MAE: 45.79\n",
      "RESULTS FOR K = 9, MANHATTEN DISTANCE\n",
      "Mean MSE: 3504.69\n",
      "Mean MAE: 47.05\n",
      "RESULTS FOR K = 9, EUCLIDEAN DISTANCE\n",
      "Mean MSE: 3451.70\n",
      "Mean MAE: 46.59\n",
      "RESULTS FOR K = 9, COSINE DISTANCE\n",
      "Mean MSE: 3403.23\n",
      "Mean MAE: 46.06\n"
     ]
    }
   ],
   "source": [
    "## perform cross-validation for a range of model hyperparameters for the Scikit-learn model ##\n",
    "K = [3,6,9]    \n",
    "for k in K:       \n",
    "    # define the pipeline for manhatten distance\n",
    "    p_manhat = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsRegressor(k, metric = 'minkowski', p = 1))])\n",
    "    # define the pipeline for euclidean distance\n",
    "    p_euclid = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsRegressor(k, metric = 'minkowski', p = 2))])\n",
    "    # define the pipeline for cosine distance\n",
    "    p_cosine = Pipeline([('scaler', StandardScaler()), ('knn', KNeighborsRegressor(k, metric = 'cosine'))])\n",
    "    # cross validate for p_manhat\n",
    "    cv_regressor_analysis(p_manhat, X, y, k, scoring_metrics, 'MANHATTEN DISTANCE')\n",
    "    # cross validate for p_euclid\n",
    "    cv_regressor_analysis(p_euclid, X, y, k, scoring_metrics, 'EUCLIDEAN DISTANCE')\n",
    "    # cross validate for p_cosine\n",
    "    cv_regressor_analysis(p_cosine, X, y, k, scoring_metrics, 'COSINE DISTANCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a58de",
   "metadata": {},
   "source": [
    "We can better summarise these results in a table:\n",
    "\n",
    "K | Distance | Custom MSE | Sklearn MSE | Custom MAE | Sklearn MAE \n",
    "--- | --- | --- | --- | --- | --- \n",
    "3 | Manhatten | 3934.75 | 3934.75 | 48.82 | 48.82 \n",
    "3 | Euclidean | 4087.34 | 4087.34 | 49.00 | 49.00\n",
    "3 | Cosine | 3814.75 | 3814.75 | 47.08 | 47.08 \n",
    "6 | Manhatten | 3598.75 | 3598.75 | 48.01 | 48.01\n",
    "6 | Euclidean | 3640.96 | 3640.96 | 47.55 | 47.55 \n",
    "6 | Cosine | 3483.80 | 3483.80 | 45.79 | 45.79\n",
    "9 | Manhatten | 3504.69 | 3504.69 | 47.05 | 47.05 \n",
    "9 | Euclidean | 3451.70 | 3451.70 | 46.59 | 46.59\n",
    "9 | Cosine | 3403.23 | 3403.23 | 46.06 | 46.06\n",
    "\n",
    "Like the situation with the KNN classifier, it is clear that our custom KNN regressor yields results that are identicial to the scikit-learn implementation. Looking at the statistics tabulated, it appears that performance improves as $K$ increases across all distance settings, with optimal values being seen for $K = 9$. Of the distance metrics attempted, the Cosine distance yields the best results for both **MSE** and **MAE**. At the same time, the Euclidean distance with $K = 3$ produces the worst set of results for these data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
