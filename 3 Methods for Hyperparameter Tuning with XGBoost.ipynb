{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "892d4c10-ff0a-490f-82c7-7f032805ad24",
   "metadata": {},
   "source": [
    "# 3 Methods for Hyperparameter Tuning with XGBoost\n",
    "\n",
    "In this notebook, we'll explore 3 different approaches for hyperparameter tuning with XGBoost. These will include:\n",
    "\n",
    "1. Grid Search\n",
    "2. Randomized Search\n",
    "3. Bayesian Optimisation\n",
    "\n",
    "There are numerous different hyperparameters available for XGBoost. A complete listing of them can be found here: https://xgboost.readthedocs.io/en/stable/parameter.html. For the purpose of our work here, I'll only consider tuning the following:\n",
    "\n",
    "* n_estimators\n",
    "* eta / learning_rate\n",
    "* max_depth\n",
    "* reg_lambda / L2 regularization\n",
    "* reg_alpha / L1 regularization\n",
    "\n",
    "For this demonstration, we will create a toy dataset using scikit-learn's *make_classification*. As such, I'll be optimizing the XGBClassifier algorithm.\n",
    "\n",
    "We can start by importing the packages necessary here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a344ae-f5d0-46a9-a742-cd28b2f93a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from scipy.stats import uniform, poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c8375-ddda-4779-b4a6-e61b216c70d6",
   "metadata": {},
   "source": [
    "Now let's create our dataset, and do a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53953757-0627-4a51-a36d-4506a59b673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in and prepare data\n",
    "X, y = make_classification(n_samples=1000, \n",
    "                           n_features=100, \n",
    "                           n_informative=60,\n",
    "                           n_classes=2, \n",
    "                           weights=[0.4,0.6],\n",
    "                           random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f626b-01ed-43ac-a627-e9b113b6707c",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "To be able to measure the effects of our tuning, let's first measure how well XGBoost does on the test set with all default values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e6482e9-8cd3-424f-a468-804bf315f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.83\n",
      "precision score: 0.81\n",
      "recall score: 0.91\n",
      "f1 score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# fit a model with default parameters\n",
    "clf = XGBClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# compute performance on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f'accuracy score: {accuracy_score(y_test, y_pred):.2f}')\n",
    "print(f\"precision score: {precision_score(y_test, y_pred):.2f}\")\n",
    "print(f\"recall score: {recall_score(y_test, y_pred):.2f}\")\n",
    "print(f\"f1 score: {f1_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38fc7e3-baaa-4307-bfe9-b16c07a77a7b",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Brute force approach to hyperparameter tuning. Each parameter configuration will be validated using 5-fold Cross-Validation. Afterwards, the best model will be selected, and tested against our held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e966ef-a496-4be3-a554-b5ccbd008cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 1000,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 1.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup parameter space\n",
    "parameters = {\n",
    "    'n_estimators':[50, 100, 200, 500, 1000],\n",
    "    'learning_rate': [0.1, 0.3, 0.6, 0.8, 1.0],\n",
    "    'max_depth':[1, 3, 6, 10],\n",
    "    'reg_alpha':[0.0, 0.1, 0.5, 1.0],\n",
    "    'reg_lambda':[0.1, 0.5, 1.0, 1.5]\n",
    "}\n",
    "\n",
    "# create an instance of the grid search object\n",
    "g = GridSearchCV(XGBClassifier(random_state=42), parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# conduct grid search over the parameter space\n",
    "start_time = time.time()\n",
    "g.fit(X_train, y_train)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# show best parameter configuration found for classifier\n",
    "cls_params = g.best_params_\n",
    "cls_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805fd7df-bdc9-4e76-889d-8df06faedde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.81\n",
      "precision score: 0.78\n",
      "recall score: 0.92\n",
      "f1 score: 0.85\n",
      "computation time: 306.76\n"
     ]
    }
   ],
   "source": [
    "# compute performance on test set\n",
    "model = g.best_estimator_\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'accuracy score: {accuracy_score(y_test,y_pred):.2f}')\n",
    "print(f'precision score: {precision_score(y_test,y_pred):.2f}')\n",
    "print(f'recall score: {recall_score(y_test,y_pred):.2f}')\n",
    "print(f'f1 score: {f1_score(y_test,y_pred):.2f}')\n",
    "print(f'computation time: {duration:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ba4c5-661b-420a-abcc-3cf7dcffb92b",
   "metadata": {},
   "source": [
    "## Randomized Search\n",
    "\n",
    "We can do hyperparameter tuning through random sampling from a probability distribution. Each parameter configuration will be validated using 5-fold Cross-Validation. Afterwards, the best model will be selected, and tested against our held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18633a5f-7156-4c11-9d3a-f62f6c0f253c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/blog/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.12063587110060081,\n",
       " 'max_depth': 5,\n",
       " 'n_estimators': 521,\n",
       " 'reg_alpha': 0.16351806389774382,\n",
       " 'reg_lambda': 1.7471572482135544}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup parameter space\n",
    "parameters = {\n",
    "    'n_estimators':poisson(mu=500),\n",
    "    'learning_rate': uniform(),\n",
    "    'max_depth':poisson(mu=6),\n",
    "    'reg_alpha':uniform(loc=0, scale=2),\n",
    "    'reg_lambda':uniform(loc=0, scale=2)\n",
    "}\n",
    "\n",
    "# create an instance of the randomized search object\n",
    "r = RandomizedSearchCV(XGBClassifier(random_state=42), parameters, cv=5, n_iter=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# conduct grid search over the parameter space\n",
    "start_time = time.time()\n",
    "r.fit(X_train,y_train)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# show best parameter configuration found for classifier\n",
    "cls_params2 = r.best_params_\n",
    "cls_params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc13652-af48-46f4-aa04-7500e3d47858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.83\n",
      "precision score: 0.80\n",
      "recall score: 0.94\n",
      "f1 score: 0.86\n",
      "computation time: 31.84\n"
     ]
    }
   ],
   "source": [
    "# compute performance on test set\n",
    "model = r.best_estimator_\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'accuracy score: {accuracy_score(y_test,y_pred):.2f}')\n",
    "print(f'precision score: {precision_score(y_test,y_pred):.2f}')\n",
    "print(f'recall score: {recall_score(y_test,y_pred):.2f}')\n",
    "print(f'f1 score: {f1_score(y_test,y_pred):.2f}')\n",
    "print(f'computation time: {duration:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1893b3-717a-4d99-a5a6-a4c91bcbe327",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "\n",
    "The final method we'll try takes advantage of Bayes theorem for hyperparameter tuning. Like before, the parameter space is defined by a set of probability distributions, in the form of priors. Care will be needed when selecting these prior distributions. Each parameter configuration will be validated using 5-fold Cross-Validation. Afterwards, the best model will be selected, and tested against our held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09652b99-3fd4-404d-b261-759ca02166b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('learning_rate', 0.15783879853890564),\n",
       "             ('max_depth', 5),\n",
       "             ('n_estimators', 550),\n",
       "             ('reg_alpha', 0.11975791012918251),\n",
       "             ('reg_lambda', 0.7717824087995475)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup parameter space\n",
    "parameters = {\n",
    "    'n_estimators':Integer(50,1000,prior='uniform'),\n",
    "    'learning_rate': Real(0.0001,1,prior='log-uniform'),\n",
    "    'max_depth':Integer(1,10,prior='uniform'),\n",
    "    'reg_alpha':Real(0.0001,2,prior='log-uniform'),\n",
    "    'reg_lambda':Real(0.0001,2,prior='log-uniform')\n",
    "}\n",
    "\n",
    "# create an instance of the bayesian search object\n",
    "b = BayesSearchCV(XGBClassifier(random_state=42), parameters, cv=5, n_iter=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# conduct randomized search over the parameter space\n",
    "start_time = time.time()\n",
    "b.fit(X_train,y_train)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "# show best parameter configuration found for classifier\n",
    "cls_params3 = b.best_params_\n",
    "cls_params3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6979b29a-18e2-4f03-9839-85c02105f6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.83\n",
      "precision score: 0.81\n",
      "recall score: 0.93\n",
      "f1 score: 0.87\n",
      "computation time: 11.31\n"
     ]
    }
   ],
   "source": [
    "# compute performance on test set\n",
    "model = b.best_estimator_\n",
    "y_pred = model.predict(X_test)\n",
    "print(f'accuracy score: {accuracy_score(y_test,y_pred):.2f}')\n",
    "print(f'precision score: {precision_score(y_test,y_pred):.2f}')\n",
    "print(f'recall score: {recall_score(y_test,y_pred):.2f}')\n",
    "print(f'f1 score: {f1_score(y_test,y_pred):.2f}')\n",
    "print(f'computation time: {duration:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bb3cf-ac90-4782-b4af-876bf9af70c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
