{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "In this notebook I will cover bootstrap aggregating, or bagging. I will use the breast cancer dataset from Article II & III to test the performance of our ensemble algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports ##\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a class to contain our ensemble. I'm going to make use of the make_bootstraps function I built in Article IV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make an ensemble classifier based on decision trees ##\n",
    "class BaggedTreeClassifier(object):\n",
    "    #initializer\n",
    "    def __init__(self,n_elements=100):\n",
    "        self.n_elements = n_elements\n",
    "        self.models     = []\n",
    "    \n",
    "    #destructor\n",
    "    def __del__(self):\n",
    "        del self.n_elements\n",
    "        del self.models\n",
    "        \n",
    "    #private function to make bootstrap samples\n",
    "    def __make_bootstraps(self,data):\n",
    "        #initialize output dictionary & unique value count\n",
    "        dc   = {}\n",
    "        unip = 0\n",
    "        #get sample size\n",
    "        b_size = data.shape[0]\n",
    "        #get list of row indexes\n",
    "        idx = [i for i in range(b_size)]\n",
    "        #loop through the required number of bootstraps\n",
    "        for b in range(self.n_elements):\n",
    "            #obtain boostrap samples with replacement\n",
    "            sidx   = np.random.choice(idx,replace=True,size=b_size)\n",
    "            b_samp = data[sidx,:]\n",
    "            #compute number of unique values contained in the bootstrap sample\n",
    "            unip  += len(set(sidx))\n",
    "            #obtain out-of-bag samples for the current b\n",
    "            oidx   = list(set(idx) - set(sidx))\n",
    "            o_samp = np.array([])\n",
    "            if oidx:\n",
    "                o_samp = data[oidx,:]\n",
    "            #store results\n",
    "            dc['boot_'+str(b)] = {'boot':b_samp,'test':o_samp}\n",
    "        #return the bootstrap results\n",
    "        return(dc)\n",
    "        \n",
    "    #train the ensemble\n",
    "    def train(self,X_train,y_train):\n",
    "        #package the input data\n",
    "        training_data = np.concatenate((X_train,y_train.reshape(-1,1)),axis=1)\n",
    "        #make bootstrap samples\n",
    "        dcBoot = self.__make_bootstraps(training_data)\n",
    "        #iterate through each bootstrap sample & fit a model ##\n",
    "        cls = DecisionTreeClassifier(class_weight='balanced')\n",
    "        for b in dcBoot:\n",
    "            #make a clone of the model\n",
    "            model = clone(cls)\n",
    "            #fit a decision tree classifier to the current sample\n",
    "            model.fit(dcBoot[b]['boot'][:,:-1],dcBoot[b]['boot'][:,-1].reshape(-1, 1))\n",
    "            #append the fitted model\n",
    "            self.models.append(model)\n",
    "            \n",
    "    #predict from the ensemble\n",
    "    def predict(self,X):\n",
    "        #check we've fit the ensemble\n",
    "        if not self.models:\n",
    "            print('You must train the ensemble before making predictions!')\n",
    "            return(None)\n",
    "        #loop through each fitted model\n",
    "        predictions = []\n",
    "        for m in self.models:\n",
    "            #make predictions on the input X\n",
    "            yp = m.predict(X)\n",
    "            #append predictions to storage list\n",
    "            predictions.append(yp.reshape(-1,1))\n",
    "        #compute the ensemble prediction\n",
    "        ypred = np.round(np.mean(np.concatenate(predictions,axis=1),axis=1)).astype(int)\n",
    "        #return the prediction\n",
    "        return(ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "\n",
    "Here I'll load the breast cancer dataset. Note I already analysed these data in Article II & III. I'll do a train-test split. The classes are unbalanced so I will have to address that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load classification dataset ##\n",
    "data = load_breast_cancer()\n",
    "X    = data.data\n",
    "y    = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do train-test split ##\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Ensemble & Produce Predictions\n",
    "\n",
    "Let's take our data and train an ensemble classifier based on decision trees. Generate predictions on the test set to check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## declare an ensemble instance with default parameters ##\n",
    "ens = BaggedTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the ensemble ##\n",
    "ens.train(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make predictions on the test set ##\n",
    "ypred = ens.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n",
      "precision: 0.99\n",
      "recall: 0.94\n"
     ]
    }
   ],
   "source": [
    "## evaluate model performance ##\n",
    "print(\"accuracy: %.2f\" % accuracy_score(y_test,ypred))\n",
    "print(\"precision: %.2f\" % precision_score(y_test,ypred))\n",
    "print(\"recall: %.2f\" % recall_score(y_test,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look good, they represent an improvement over the lone decision tree classifier (see results from Article III). This is especially true in terms of the accuracy of the model predictions. Note that no hyperparameter tuning was done: further improvements to our ensemble are therefore possible.\n",
    "\n",
    "Let's compare how our custom-built ensemble compares to the one available through scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the scikit-learn model ##\n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## declare a bagging classifier instance ##\n",
    "ens = BaggingClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(n_estimators=100)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the ensemble ##\n",
    "ens.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make predictions on the test set ##\n",
    "ypred = ens.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.96\n",
      "precision: 0.99\n",
      "recall: 0.94\n"
     ]
    }
   ],
   "source": [
    "## evaluate model performance ##\n",
    "print(\"accuracy: %.2f\" % accuracy_score(y_test,ypred))\n",
    "print(\"precision: %.2f\" % precision_score(y_test,ypred))\n",
    "print(\"recall: %.2f\" % recall_score(y_test,ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results here are in agreement with those obtained with our custom ensemble classifier. Like the case with the custom classifier, no hyperparameter tuning was done here. Further work to optimise the hyperparameters of the base decision tree classifier could offer further improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
